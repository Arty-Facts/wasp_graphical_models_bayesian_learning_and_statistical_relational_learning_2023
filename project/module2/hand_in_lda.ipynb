{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "\n",
    "In this part of the assignment you will analyze a data set consisting of abstracts from the 2017 Conference on Neural Information Processing Systems (NeurIPS), The file `papers2017.csv` contans a list of 679 titles and abstracts from the conference proceedings. \n",
    "\n",
    "The task is to compute the posterior distribution in the LDA model that was discussed during the lectures. Set the hyperparameters of the prior to $\\alpha = \\eta = 1$. The number of topics can be set to $K = 5$.\n",
    "\n",
    "Before we can begin with the inference we need to load and pre-process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Arturas.Aleksandraus\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import scipy\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "stemmer = SnowballStemmer('english')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "documents = pd.read_csv('papers2017.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process the data\n",
    "\n",
    "* **Tokenization:** Split each headline into words, lowercase the words and remove punctuation.\n",
    "* **Small words:** Remove all words with less than 3 characters.\n",
    "* **Stopwords:** Remove all stopwords.\n",
    "* **Lemmatize:** Change words in third person to first person and all verbs into present.\n",
    "* **Stem:** Reduce the words to their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizing and stemming\n",
    "def lem_stem(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text,pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in STOPWORDS and len(token) >= 3:\n",
    "            result.append(lem_stem(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to process all of the abstract through this pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['abstract'].map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we create our dictionary by filtering out words with less than 10 appearances and all words which appears in more than half of the documents.\n",
    "\n",
    "We then create a bag of words, where each processed document gets replaced by a list of words and the number of times they appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collapsed Gibbs\n",
    "\n",
    "**Q1:**  For a sweep of the collapsed Gibbs sampler we need to sample from the distribution $p(\\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\eta)$. Where $\\mathbf{z}$ is the topic per word for the documents.\n",
    "\n",
    "Write out the full distribution and simplify the computations as much as possible. \n",
    "\n",
    "Also explain how to obtain the posterior distribution from the variables $\\mathbf{\\theta}$ (topic proportions for each document) and $\\beta$ (word distributions for each topic), based on the output of the Gibbs sampler. That is how do we find the distribution of these quantities given the distribution of $\\mathbf{z}$.\n",
    "\n",
    "_hint: see the slides_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can analytic compute $\\theta$ and $\\beta$ and only sample $z$. The resulting marginalization leads to a simplify objective function that looks s following:\n",
    "$$\n",
    "p(z_{d,n} = k \\mid \\mathbf{z}^{-(d,n)}, \\mathbf{w}, \\alpha, \\eta) \\propto p(z_{d,n} = k \\mid \\mathbf{z}^{-(d,n)}, \\alpha) \\cdot p(w_{d,n} \\mid z_{d,n} = k, \\mathbf{w}^{-(d,n)}, \\mathbf{z}^{-(d,n)}, \\eta) \\propto \\hat{\\theta}_{d,k} \\hat{\\beta}_{k,v}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{d,k} = \\frac{\\alpha + c_{d,k}}{\\sum_{k'=1}^{K}(\\alpha + c_{d,k'})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{k,v} = \\frac{\\eta + \\tilde{c}_{k,v}}{\\sum_{v'=1}^{V}(\\eta + \\tilde{c}_{k,v'})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2:** Implement the collapsed Gibbs sampler on the provided NeurIPS dataset to approximate the posterior $p(\\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\eta)$. To help with this you have code providing the outer loop, you need to implement the initialization algorithm and the function for calculating the probabilities of the posterior distribution.\n",
    "\n",
    "Where `c` and `ct` are $c$ and $\\tilde{c}$ from the slides. The count of each topic per document and the count of each topic per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta(v, k, topic_word, eta):\n",
    "    return (topic_word[k][v] + eta) / (topic_word[k] + eta).sum()\n",
    "\n",
    "def theta(d, k, doc_topic, alpha):\n",
    "    return (doc_topic[d][k] + alpha) / (doc_topic[d] + alpha).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization function, sets the topic for each word in each document at random.\n",
    "def initialization_gibbs(bow_corpus, num_topics, num_words):\n",
    "    num_docs = len(bow_corpus)\n",
    "    z = []\n",
    "    c = np.zeros((num_docs,num_topics), dtype = int) # c[d][k] = number of words in document d assigned to topic k\n",
    "    ct = np.zeros((num_topics,num_words), dtype = int)\n",
    "    for d in range(num_docs):\n",
    "        topics_in_doc = np.zeros(0,dtype = int)\n",
    "        for (v,i) in bow_corpus[d]:\n",
    "            # Inner loop to set topic for each copy of a word independently.\n",
    "            for _ in range(i):\n",
    "                # Sample a random topic\n",
    "                k = np.random.randint(num_topics) # how do we sample from categorical distribution? Uniform?\n",
    "                # Update list of topics in document\n",
    "                topics_in_doc = np.append(topics_in_doc,k)\n",
    "\n",
    "                # Update c and ct based on document d, word v and topic k\n",
    "                c[d][k] += 1\n",
    "                ct[k][v] += 1\n",
    "        z.append(topics_in_doc)\n",
    "    return(z, c, ct)\n",
    "# Calculates the probability for each topic on the current word and document.\n",
    "def calc_probs(c, ct, d, v, alpha, eta, num_topics):\n",
    "    p = np.zeros(num_topics)\n",
    "\n",
    "    for k in range(num_topics):\n",
    "        # For each topic calculate the probability that word v in document d belongs to topic k.\n",
    "        p[k] = theta(d, k, c, alpha) *  beta(v, k, ct, eta)\n",
    "    # Normalize the vector before returning it.\n",
    "    return p/sum(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "eta = 0.1\n",
    "\n",
    "# Number of iterations (number of samples from the posterior)\n",
    "max_itr_gibbs = 10\n",
    "\n",
    "# Number of topics in the model\n",
    "num_topics = 10\n",
    "\n",
    "# Number of words and documents (may help you later)\n",
    "num_words = len(dictionary)\n",
    "num_docs = len(bow_corpus)\n",
    "\n",
    "# Start by initializing all values: z (topics) should be set randomly and c and ct (tilde c) should be calculated based on the randomly set topics.\n",
    "(z, c, ct) = initialization_gibbs(bow_corpus, num_topics, num_words)\n",
    "\n",
    "# Gibbs sampling:\n",
    "for itr in range(max_itr_gibbs):\n",
    "    for d in range(num_docs):\n",
    "        # indx keeps track of the index of the words in each document\n",
    "        indx = 0\n",
    "        for (v,i) in bow_corpus[d]:\n",
    "            for _ in range(i):\n",
    "                # k is current topic\n",
    "                k = z[d][indx]\n",
    "\n",
    "                # Decrease c and ct based on the current topic\n",
    "                c[d][k] -= 1\n",
    "                ct[k][v] -= 1\n",
    "\n",
    "                # Calculate probabilities for the posterior distribution\n",
    "                probs = calc_probs(c, ct, d, v, alpha, eta, num_topics)\n",
    "                \n",
    "\n",
    "                # Sample new topic\n",
    "                new_k = np.random.choice(num_topics, p = probs)\n",
    "\n",
    "                # Increase c and ct based on the new topic\n",
    "                c[d][new_k] += 1\n",
    "                ct[new_k][v] += 1\n",
    "\n",
    "                # Set the word (index indx) to the new topic\n",
    "                z[d][indx] = new_k\n",
    "                indx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3:** Present the top 5 words based on term-score for each topic, also give a name to each of the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0: optim, convex, converg, gradient, descent\n",
      "topic 1: imag, generat, distribut, task, discrimin\n",
      "topic 2: set, multi, algorithm, comput, noisi\n",
      "topic 3: imag, neural, fair, train, action\n",
      "topic 4: network, layer, task, deep, activ\n",
      "topic 5: regret, function, algorithm, time, polynomi\n",
      "topic 6: cluster, distribut, analysi, class, empir\n",
      "topic 7: train, game, infer, variabl, converg\n",
      "topic 8: data, robust, generat, real, featur\n",
      "topic 9: dynam, matrix, size, structur, empir\n"
     ]
    }
   ],
   "source": [
    "def term_score(v, k, topic_word, eta, num_topics):  \n",
    "    bkv = beta(v, k, topic_word, eta)\n",
    "    log_beta = np.log(bkv)\n",
    "    log_sum_beta = sum([np.log(beta(v, t, topic_word, eta)) for t in range(num_topics)])\n",
    "    return bkv*(log_beta - log_sum_beta/num_topics)\n",
    "\n",
    "def rank_words_per_topic(topic_word, eta, num_topics, num_words, top = 5):\n",
    "    score_word = []\n",
    "    for k in range(num_topics):\n",
    "        topic_score = []\n",
    "        for v in range(num_words):\n",
    "            topic_score.append((term_score(v, k, topic_word, eta, num_topics), v))\n",
    "        topic_score.sort(reverse = True)\n",
    "        score_word.append(topic_score)\n",
    "    word = [[word for score, word in topic][:top] for topic in score_word]\n",
    "    return word\n",
    "\n",
    "for topic, words in enumerate(rank_words_per_topic(ct, eta, num_topics, num_words)):\n",
    "    print(f'topic {topic}: ' + ', '.join([dictionary[id] for id in words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational inference\n",
    "\n",
    "**Q4:** Write down explicit update expressions for a CAVI algorithm for approximating the posterior distribution $p(\\mathbf{\\theta},\\mathbf{z},\\beta \\mid \\mathbf{w}, \\alpha, \\eta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\phi_{d,n}^k \\propto exp(\\psi(\\gamma_{d,k}) + \\psi(\\lambda_{k,{w_{d,n}}}) - \\psi(\\sum_v \\lambda_{k,v})))\n",
    "$$\n",
    "$$\n",
    "\\gamma_{d} = \\alpha + \\sum_n \\phi_{d,n}\\\\\n",
    "\\lambda_{k} = \\eta + \\sum_d \\sum_n \\mathbb{1}\\{w_{d,n}=v\\}\\phi_{d,n}^k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5:** Implement the CAVI algorithm for the NeurIPS data. To help you with this task some code is provided that you need to fill in.\n",
    "\n",
    "For the code we work with $\\log(\\phi)$, this simplifies some of the expressions and is in general more numericly stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilization function, sets lambdas and gammas randomly, sets all phis to 0.\n",
    "def initialization_cavi(bow_corpus, num_topics, num_words):\n",
    "    num_docs = len(bow_corpus)\n",
    "    logphis = []\n",
    "    wordMatrix = []\n",
    "    docLengths = []\n",
    "    lambdas = np.random.gamma(1, size = (num_topics, num_words))\n",
    "    gammas = np.random.gamma(1, size = (num_docs, num_topics))\n",
    "    for d in range(num_docs):\n",
    "        words_in_doc = 0\n",
    "        word_vec = np.zeros(0, dtype = int)\n",
    "        for (v,i) in bow_corpus[d]:\n",
    "            words_in_doc += i\n",
    "            for _ in range(i):\n",
    "                word_vec = np.append(word_vec, v)\n",
    "        logphis.append(np.zeros((words_in_doc,num_topics)))\n",
    "        docLengths.append(words_in_doc)\n",
    "        wordMatrix.append(word_vec)\n",
    "    return(logphis, lambdas, gammas, docLengths, wordMatrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "eta = 0.1\n",
    "\n",
    "# Number of iterations (number of samples from the posterior)\n",
    "max_itr_cavi = 10\n",
    "\n",
    "# Number of topics in the model\n",
    "num_topics = 10\n",
    "\n",
    "# Number of words and documents (may help you later)\n",
    "num_words = len(dictionary)\n",
    "num_docs = len(bow_corpus)\n",
    "\n",
    "# Start by initializing all values, we set all phis to zero and randomize lambdas and gammas from the gamma distribution. Also calculates doc_lengths and word_matrix.\n",
    "(logphis, lambdas, gammas, doc_lengths, word_matrix) = initialization_cavi(bow_corpus, num_topics, num_words)\n",
    "\n",
    "for itr in range(max_itr_cavi):\n",
    "    for d in range(num_docs):\n",
    "        indx = 0\n",
    "        for (v,i) in bow_corpus[d]:\n",
    "            for _ in range(i):\n",
    "                for k in range(num_topics):\n",
    "                    # Calculate each logphi based on the expected value of the natural parametrization.\n",
    "                    # The digamma function is available in scipy.specieal.digamma\n",
    "                    logphis[d][indx][k] = scipy.special.digamma(gammas[d, k]) + scipy.special.digamma(lambdas[k, v]) - scipy.special.digamma(lambdas[k, :].sum())\n",
    "                # Normalize the logphis                                                                                 \n",
    "                logphis[d][indx, :] = logphis[d][indx, :] - np.log(np.exp(logphis[d][indx, :]).sum())\n",
    "                indx += 1\n",
    "        for k in range(num_topics):\n",
    "            # Calculate the gammas based on the phis\n",
    "            gammas[d][k] = alpha + np.exp(logphis[d][:, k]).sum()\n",
    "    for k in range(num_topics):\n",
    "        for v in range(num_words):\n",
    "            # Update the lambdas. \n",
    "            lambdas[k][v] = eta + np.sum([np.exp(logphis[d][word_matrix[d] == v, k]).sum() for d in range(num_docs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6:** Present the top-5 words based on term-score for each topic and also give a name to each of the topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0: structur, function, convex, set, discrimin\n",
      "topic 1: polici, approxim, train, agent, high\n",
      "topic 2: data, sampl, method, convex, stochast\n",
      "topic 3: optim, approxim, linear, sampl, converg\n",
      "topic 4: data, train, network, deep, imag\n",
      "topic 5: algorithm, problem, label, time, number\n",
      "topic 6: task, problem, result, estim, object\n",
      "topic 7: infer, data, method, complex, approach\n",
      "topic 8: network, neural, featur, train, data\n",
      "topic 9: regret, algorithm, neural, estim, featur\n"
     ]
    }
   ],
   "source": [
    "for topic, words in enumerate(rank_words_per_topic(lambdas, eta, num_topics, num_words)):\n",
    "    print(f'topic {topic}: ' + ', '.join([dictionary[id] for id in words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Gibbs and Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7:** Choose one of the abstracts, present the top 5 topics of the document and present the title of the 5 closest other abstracts. Do this for both of the algorithms on the same abstract. Discuss similairities and differences between the results from the two algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning\n",
      "Abstract: Long Short-Term Memory (LSTM) is a popular approach to boosting the ability of Recurrent Neural Networks to store longer term temporal information. The capacity of an LSTM network can be increased by widening and adding layers. However, usually the former introduces additional parameters, while the latter increases the runtime. As an alternative we propose the Tensorized LSTM in which the hidden states are represented by tensors and updated via a cross-layer convolution. By increasing the tensor size, the network can be widened efficiently without additional parameters since the parameters are shared across different locations in the tensor; by delaying the output, the network can be deepened implicitly with little additional runtime since deep computations for each timestep are merged into temporal computations of the sequence. Experiments conducted on five challenging sequence learning tasks show the potential of the proposed model.\n",
      "Gibbs Topics:\n",
      "topic 4, aliment: 0.39, network, layer, task, deep, activ\n",
      "topic 8, aliment: 0.36, data, robust, generat, real, featur\n",
      "topic 2, aliment: 0.19, set, multi, algorithm, comput, noisi\n",
      "topic 7, aliment: 0.03, train, game, infer, variabl, converg\n",
      "topic 5, aliment: 0.03, regret, function, algorithm, time, polynomi\n",
      "Gibbs Document Similarity:\n",
      "0.8637931034482758\n",
      "Doc 491, similarity: 0.12, title: Learning Graph Representations with Embedding Propagation\n",
      "Doc 79, similarity: 0.13, title: Label Distribution Learning Forests\n",
      "Doc 151, similarity: 0.14, title: Learning Multiple Tasks with Multilinear Relationship Networks\n",
      "Doc 265, similarity: 0.18, title: Premise Selection for Theorem Proving by Deep Graph Embedding\n",
      "Doc 91, similarity: 0.23, title: Decoupling \"when to update\" from \"how to update\"\n",
      "\n",
      "CAVI Topics:\n",
      "topic 9, aliment: 0.33, regret, algorithm, neural, estim, featur\n",
      "topic 4, aliment: 0.31, data, train, network, deep, imag\n",
      "topic 3, aliment: 0.27, optim, approxim, linear, sampl, converg\n",
      "topic 6, aliment: 0.07, task, problem, result, estim, object\n",
      "topic 0, aliment: 0.01, structur, function, convex, set, discrimin\n",
      "CAVI Document similarity:\n",
      "0.729681850022255\n",
      "Doc 193, similarity: 0.10, title: Data-Efficient Reinforcement Learning in Continuous State-Action Gaussian-POMDPs\n",
      "Doc 460, similarity: 0.13, title: Scalable Variational Inference for Dynamical Systems\n",
      "Doc 114, similarity: 0.17, title: Matching neural paths: transfer from recognition to correspondence search\n",
      "Doc 87, similarity: 0.20, title: Online multiclass boosting\n",
      "Doc 423, similarity: 0.22, title: Federated Multi-Task Learning\n"
     ]
    }
   ],
   "source": [
    "def topics_for_doc(doc_id, doc_topics, topic_word, eta, num_topics, num_words, top = 5):\n",
    "    topic_dist = doc_topics[doc_id]/doc_topics[doc_id].sum()\n",
    "    # Get topical words for each topic\n",
    "    topic_words = rank_words_per_topic(topic_word, eta, num_topics, num_words, top)\n",
    "    # Sort the topics based on the topic alignment\n",
    "    topic_dist_words = sorted(zip(topic_dist, range(num_topics), topic_words), reverse = True)\n",
    "    return topic_dist_words[:top]\n",
    "\n",
    "def document_similarity(doc_id, doc_topics, alpha, top = 5):\n",
    "    thetas = np.zeros_like(doc_topics, dtype = np.float64)\n",
    "    # Calculate the thetas for each document\n",
    "    for d in range(num_docs):\n",
    "        for k in range(num_topics):\n",
    "            thetas[d][k] = theta(d, k, doc_topics, alpha)\n",
    "    # compute the similarity between the document and all other documents\n",
    "    print(thetas.max())\n",
    "    thetas = np.sqrt(thetas)\n",
    "    thetas -= thetas[doc_id]\n",
    "    thetas = thetas**2\n",
    "    similarity = thetas.sum(axis = 1)\n",
    "    similarity[doc_id] = np.inf # Set the similarity to itself to infinity so its not returned\n",
    "    # Sort the documents based on similarity\n",
    "    similarity_index = np.argsort(similarity)\n",
    "    #return the top similar documents\n",
    "    return list(zip(similarity[similarity_index][:top], similarity_index[:top]))\n",
    "    \n",
    "\n",
    "document = 0\n",
    "title = documents.iloc[document]['title']\n",
    "abstract = documents.iloc[document]['abstract']\n",
    "\n",
    "print(f'Title: {title}')\n",
    "print(f'Abstract: {abstract}')\n",
    "\n",
    "print('Gibbs Topics:')\n",
    "for topic_score, topic_id, words in topics_for_doc(document, c, ct, eta, num_topics, num_words):\n",
    "    print(f'topic {topic_id}, aliment: {topic_score:.2f}, ' + ', '.join([dictionary[id] for id in words]))\n",
    "print('Gibbs Document Similarity:')\n",
    "for similarity, doc_id in document_similarity(document, c, alpha):\n",
    "    print(f'Doc {doc_id}, similarity: {similarity:.2f}, title: {documents.iloc[doc_id][\"title\"]}')\n",
    "print()\n",
    "print('CAVI Topics:')\n",
    "for topic_score, topic_id, words in topics_for_doc(document, gammas, lambdas, eta, num_topics, num_words):\n",
    "    print(f'topic {topic_id}, aliment: {topic_score:.2f}, ' + ', '.join([dictionary[id] for id in words]))\n",
    "print('CAVI Document similarity:')\n",
    "for similarity, doc_id in document_similarity(document, gammas, alpha):\n",
    "    print(f'Doc {doc_id}, similarity: {similarity:.2f}, title: {documents.iloc[doc_id][\"title\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8:** Discuss the key conceptual differences between the Gibbs sampler and the CAVI algorithm. What are the pros and cons of each method? Which method do you prefer and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are both approximate Bayesian inference methods, however\n",
    "\n",
    "## Gibbs sampler \n",
    "\n",
    "Gibbs sampler is a **Markov Chain Monte Carlo** method, non-parametric and sampling-based approximation.It generates samples from the posterior distribution by sequentially sampling each variable conditioned on the current values of the other variables.\n",
    "\n",
    "### Pros\n",
    "\n",
    "* Gibbs sampling eventually converges to the true posterior distribution, given enough iterations.\n",
    "\n",
    "### Cons\n",
    "\n",
    "* Not suitable for high-dimensional spaces since it conditioned on the current values of the other variables.\n",
    "\n",
    "* Samples might be correlated\n",
    "\n",
    "## CAVI algorithm\n",
    "\n",
    "CAVI algorithm is a **Variational inference** method, parametric (“deterministic”) approximation. It approximates the posterior distribution by a simpler, factorizable distribution. CAVI iteratively updates each factor of the approximation to maximize ELBO.\n",
    "\n",
    "### Pros\n",
    "\n",
    "* Very efficient \n",
    "\n",
    "* Suitable for high-dimensional spaces\n",
    "\n",
    "### Cons\n",
    "\n",
    "* CAVI can at most approximate a lower bound of the true posterior distribution. In some cases, posterior will be hard to approximate.\n",
    "\n",
    "* Limited to models where the posterior can be approximated by a factorizable distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
