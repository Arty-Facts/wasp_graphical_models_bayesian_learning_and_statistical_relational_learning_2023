{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "\n",
    "In this part of the assignment you will analyze a data set consisting of abstracts from the 2017 Conference on Neural Information Processing Systems (NeurIPS), The file `papers2017.csv` contans a list of 679 titles and abstracts from the conference proceedings. \n",
    "\n",
    "The task is to compute the posterior distribution in the LDA model that was discussed during the lectures. Set the hyperparameters of the prior to $\\alpha = \\eta = 1$. The number of topics can be set to $K = 5$.\n",
    "\n",
    "Before we can begin with the inference we need to load and pre-process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Arturas.Aleksandraus\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import scipy\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "stemmer = SnowballStemmer('english')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "documents = pd.read_csv('papers2017.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process the data\n",
    "\n",
    "* **Tokenization:** Split each headline into words, lowercase the words and remove punctuation.\n",
    "* **Small words:** Remove all words with less than 3 characters.\n",
    "* **Stopwords:** Remove all stopwords.\n",
    "* **Lemmatize:** Change words in third person to first person and all verbs into present.\n",
    "* **Stem:** Reduce the words to their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizing and stemming\n",
    "def lem_stem(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text,pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in STOPWORDS and len(token) >= 3:\n",
    "            result.append(lem_stem(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to process all of the abstract through this pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['abstract'].map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we create our dictionary by filtering out words with less than 10 appearances and all words which appears in more than half of the documents.\n",
    "\n",
    "We then create a bag of words, where each processed document gets replaced by a list of words and the number of times they appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collapsed Gibbs\n",
    "\n",
    "**Q1:**  For a sweep of the collapsed Gibbs sampler we need to sample from the distribution $p(\\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\eta)$. Where $\\mathbf{z}$ is the topic per word for the documents.\n",
    "\n",
    "Write out the full distribution and simplify the computations as much as possible. \n",
    "\n",
    "Also explain how to obtain the posterior distribution from the variables $\\mathbf{\\theta}$ (topic proportions for each document) and $\\beta$ (word distributions for each topic), based on the output of the Gibbs sampler. That is how do we find the distribution of these quantities given the distribution of $\\mathbf{z}$.\n",
    "\n",
    "_hint: see the slides_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can analytic compute $\\theta$ and $\\beta$ and only sample $z$. The resulting marginalization leads to a simplify objective function that looks s following:\n",
    "$$\n",
    "p(z_{d,n} = k \\mid \\mathbf{z}^{-(d,n)}, \\mathbf{w}, \\alpha, \\eta) \\propto p(z_{d,n} = k \\mid \\mathbf{z}^{-(d,n)}, \\alpha) \\cdot p(w_{d,n} \\mid z_{d,n} = k, \\mathbf{w}^{-(d,n)}, \\mathbf{z}^{-(d,n)}, \\eta) \\propto \\hat{\\theta}_{d,k} \\hat{\\beta}_{k,v}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{d,k} = \\frac{\\alpha + c_{d,k}}{\\sum_{k'=1}^{K}(\\alpha + c_{d,k'})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{k,v} = \\frac{\\eta + \\tilde{c}_{k,v}}{\\sum_{v'=1}^{V}(\\eta + \\tilde{c}_{k,v'})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2:** Implement the collapsed Gibbs sampler on the provided NeurIPS dataset to approximate the posterior $p(\\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\eta)$. To help with this you have code providing the outer loop, you need to implement the initialization algorithm and the function for calculating the probabilities of the posterior distribution.\n",
    "\n",
    "Where `c` and `ct` are $c$ and $\\tilde{c}$ from the slides. The count of each topic per document and the count of each topic per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization function, sets the topic for each word in each document at random.\n",
    "def initialization_gibbs(bow_corpus, num_topics, num_words):\n",
    "    num_docs = len(bow_corpus)\n",
    "    z = []\n",
    "    c = np.zeros((num_docs,num_topics), dtype = int) # c[d][k] = number of words in document d assigned to topic k\n",
    "    ct = np.zeros((num_topics,num_words), dtype = int)\n",
    "    for d in range(num_docs):\n",
    "        topics_in_doc = np.zeros(0,dtype = int)\n",
    "        for (v,i) in bow_corpus[d]:\n",
    "            # Inner loop to set topic for each copy of a word independently.\n",
    "            for _ in range(i):\n",
    "                # Sample a random topic\n",
    "                k = np.random.randint(num_topics) # how do we sample from categorical distribution? Uniform?\n",
    "                # Update list of topics in document\n",
    "                topics_in_doc = np.append(topics_in_doc,k)\n",
    "\n",
    "                # Update c and ct based on document d, word v and topic k\n",
    "                c[d][k] += 1\n",
    "                ct[k][v] += 1\n",
    "        z.append(topics_in_doc)\n",
    "    return(z, c, ct)\n",
    "# Calculates the probability for each topic on the current word and document.\n",
    "def calc_probs(c, ct, d, v, alpha, eta, num_topics):\n",
    "    p = np.zeros(num_topics)\n",
    "\n",
    "    for k in range(num_topics):\n",
    "        # For each topic calculate the probability that word v in document d belongs to topic k.\n",
    "        theta = (c[d][k] + alpha)/ (c[d].sum() * alpha) \n",
    "        beta = (ct[k][v] + eta) / (ct[k].sum() * eta)\n",
    "        p[k] = theta * beta\n",
    "    # Normalize the vector before returning it.\n",
    "    return p/sum(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "eta = 0.1\n",
    "\n",
    "# Number of iterations (number of samples from the posterior)\n",
    "max_itr_gibbs = 10\n",
    "\n",
    "# Number of topics in the model\n",
    "num_topics = 10\n",
    "\n",
    "# Number of words and documents (may help you later)\n",
    "num_words = len(dictionary)\n",
    "num_docs = len(bow_corpus)\n",
    "\n",
    "# Start by initializing all values: z (topics) should be set randomly and c and ct (tilde c) should be calculated based on the randomly set topics.\n",
    "(z, c, ct) = initialization_gibbs(bow_corpus, num_topics, num_words)\n",
    "\n",
    "# Gibbs sampling:\n",
    "for itr in range(max_itr_gibbs):\n",
    "    for d in range(num_docs):\n",
    "        # indx keeps track of the index of the words in each document\n",
    "        indx = 0\n",
    "        for (v,i) in bow_corpus[d]:\n",
    "            for _ in range(i):\n",
    "                # k is current topic\n",
    "                k = z[d][indx]\n",
    "\n",
    "                # Decrease c and ct based on the current topic\n",
    "                c[d][k] -= 1\n",
    "                ct[k][v] -= 1\n",
    "\n",
    "                # Calculate probabilities for the posterior distribution\n",
    "                probs = calc_probs(c, ct, d, v, alpha, eta, num_topics)\n",
    "                \n",
    "\n",
    "                # Sample new topic\n",
    "                new_k = np.random.choice(num_topics, p = probs)\n",
    "\n",
    "                # Increase c and ct based on the new topic\n",
    "                c[d][new_k] += 1\n",
    "                ct[new_k][v] += 1\n",
    "\n",
    "                # Set the word (index indx) to the new topic\n",
    "                z[d][indx] = new_k\n",
    "                indx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3:** Present the top 5 words based on term-score for each topic, also give a name to each of the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0: state, art, base, method, predict\n",
      "topic 1: data, train, latent, imag, condit\n",
      "topic 2: polici, valu, reinforc, agent, critic\n",
      "topic 3: random, algorithm, linear, signal, problem\n",
      "topic 4: infer, variat, distribut, bayesian, variabl\n",
      "topic 5: sampl, comput, distribut, cost, number\n",
      "topic 6: converg, convex, gradient, stochast, descent\n",
      "topic 7: method, larg, effici, memori, perform\n",
      "topic 8: class, probabl, bound, oracl, set\n",
      "topic 9: function, non, optim, submodular, distribut\n",
      "topic 10: estim, matrix, rank, kernel, error\n",
      "topic 11: task, domain, set, demonstr, nois\n",
      "topic 12: fair, data, problem, method, causal\n",
      "topic 13: function, optim, loss, problem, bound\n",
      "topic 14: object, end, set, invari, constraint\n",
      "topic 15: regret, bandit, action, game, arm\n",
      "topic 16: network, human, prune, physic, state\n",
      "topic 17: data, featur, subset, label, select\n",
      "topic 18: famili, distribut, general, rule, design\n",
      "topic 19: imag, method, approach, visual, map\n",
      "topic 20: predict, futur, recurr, multi, represent\n",
      "topic 21: test, result, featur, algorithm, addit\n",
      "topic 22: generat, train, label, adversari, discrimin\n",
      "topic 23: kernel, time, algorithm, approxim, random\n",
      "topic 24: converg, gin, optim, gradient, generat\n",
      "topic 25: neural, dynam, activ, time, neuron\n",
      "topic 26: network, deep, train, neural, layer\n",
      "topic 27: cluster, point, similar, queri, method\n",
      "topic 28: approach, real, data, evalu, perform\n",
      "topic 29: observ, estim, attent, sequenc, discret\n",
      "topic 30: network, convolut, represent, dataset, neural\n",
      "topic 31: algorithm, problem, optim, onlin, regret\n"
     ]
    }
   ],
   "source": [
    "def beta(v, k, topic_word, eta):\n",
    "    return (topic_word[k][v] + eta) / (topic_word[k].sum() + eta)\n",
    "\n",
    "def term_score(v, k, topic_word, eta, num_topics):  \n",
    "    bkv = beta(v, k, topic_word, eta)\n",
    "    log_beta = np.log(bkv)\n",
    "    log_sum_beta = sum([np.log(beta(v, t, topic_word, eta)) for t in range(num_topics)])\n",
    "    return bkv*(log_beta - log_sum_beta/num_topics)\n",
    "\n",
    "def rank_words_per_topic(topic_word, eta, num_topics, num_words, top = 5):\n",
    "    score_word = []\n",
    "    for k in range(num_topics):\n",
    "        topic_score = []\n",
    "        for v in range(num_words):\n",
    "            topic_score.append((term_score(v, k, topic_word, eta, num_topics), v))\n",
    "        topic_score.sort(reverse = True)\n",
    "        score_word.append(topic_score)\n",
    "    word = [[word for score, word in topic][:top] for topic in score_word]\n",
    "    return word\n",
    "\n",
    "for topic, words in enumerate(rank_words_per_topic(ct, eta, num_topics, num_words)):\n",
    "    print(f'topic {topic}: ' + ', '.join([dictionary[id] for id in words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational inference\n",
    "\n",
    "**Q4:** Write down explicit update expressions for a CAVI algorithm for approximating the posterior distribution $p(\\mathbf{\\theta},\\mathbf{z},\\beta \\mid \\mathbf{w}, \\alpha, \\eta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\phi_{d,n}^k \\propto exp(\\psi(\\gamma_{d,k}) + \\psi(\\lambda_{k,{w_{d,n}}}) - \\psi(\\sum_v \\lambda_{k,v})))\n",
    "$$\n",
    "$$\n",
    "\\gamma_{d} = \\alpha + \\sum_n \\phi_{d,n}\\\\\n",
    "\\lambda_{k} = \\eta + \\sum_d \\sum_n \\mathbb{1}\\{w_{d,n}=v\\}\\phi_{d,n}^k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5:** Implement the CAVI algorithm for the NeurIPS data. To help you with this task some code is provided that you need to fill in.\n",
    "\n",
    "For the code we work with $\\log(\\phi)$, this simplifies some of the expressions and is in general more numericly stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilization function, sets lambdas and gammas randomly, sets all phis to 0.\n",
    "def initialization_cavi(bow_corpus, num_topics, num_words):\n",
    "    num_docs = len(bow_corpus)\n",
    "    logphis = []\n",
    "    wordMatrix = []\n",
    "    docLengths = []\n",
    "    lambdas = np.random.gamma(1, size = (num_topics, num_words))\n",
    "    gammas = np.random.gamma(1, size = (num_docs, num_topics))\n",
    "    for d in range(num_docs):\n",
    "        words_in_doc = 0\n",
    "        word_vec = np.zeros(0, dtype = int)\n",
    "        for (v,i) in bow_corpus[d]:\n",
    "            words_in_doc += i\n",
    "            for _ in range(i):\n",
    "                word_vec = np.append(word_vec, v)\n",
    "        logphis.append(np.zeros((words_in_doc,num_topics)))\n",
    "        docLengths.append(words_in_doc)\n",
    "        wordMatrix.append(word_vec)\n",
    "    return(logphis, lambdas, gammas, docLengths, wordMatrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "eta = 0.1\n",
    "\n",
    "# Number of iterations (number of samples from the posterior)\n",
    "max_itr_cavi = 10\n",
    "\n",
    "# Number of topics in the model\n",
    "num_topics = 10\n",
    "\n",
    "# Number of words and documents (may help you later)\n",
    "num_words = len(dictionary)\n",
    "num_docs = len(bow_corpus)\n",
    "\n",
    "# Start by initializing all values, we set all phis to zero and randomize lambdas and gammas from the gamma distribution. Also calculates doc_lengths and word_matrix.\n",
    "(logphis, lambdas, gammas, doc_lengths, word_matrix) = initialization_cavi(bow_corpus, num_topics, num_words)\n",
    "\n",
    "for itr in range(max_itr_cavi):\n",
    "    for d in range(num_docs):\n",
    "        indx = 0\n",
    "        for (v,i) in bow_corpus[d]:\n",
    "            for _ in range(i):\n",
    "                for k in range(num_topics):\n",
    "                    # Calculate each logphi based on the expected value of the natural parametrization.\n",
    "                    # The digamma function is available in scipy.specieal.digamma\n",
    "                    logphis[d][indx][k] = scipy.special.digamma(gammas[d, k]) + scipy.special.digamma(lambdas[k, v]) - scipy.special.digamma(lambdas[k, :].sum())\n",
    "                # Normalize the logphis                                                                                 \n",
    "                logphis[d][indx, :] = logphis[d][indx, :] - np.log(np.exp(logphis[d][indx, :]).sum())\n",
    "                indx += 1\n",
    "        for k in range(num_topics):\n",
    "            # Calculate the gammas based on the phis\n",
    "            gammas[d][k] = alpha + np.exp(logphis[d][:, k]).sum()\n",
    "    for k in range(num_topics):\n",
    "        for v in range(num_words):\n",
    "            # Update the lambdas. \n",
    "            lambdas[k][v] = eta + np.sum([np.exp(logphis[d][word_matrix[d] == v, k]).sum() for d in range(num_docs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6:** Present the top-5 words based on term-score for each topic and also give a name to each of the topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0: algorithm, estim, set, comput, graph\n",
      "topic 1: inform, task, imag, sequenc, likelihood\n",
      "topic 2: network, sampl, distribut, neural, complex\n",
      "topic 3: optim, problem, gradient, larg, approxim\n",
      "topic 4: time, result, latent, imag, variabl\n",
      "topic 5: neural, network, train, data, deep\n",
      "topic 6: data, rank, estim, generat, matrix\n",
      "topic 7: data, method, object, random, represent\n",
      "topic 8: task, predict, set, dynam, differ\n",
      "topic 9: imag, label, supervis, gin, featur\n",
      "topic 10: converg, function, convex, problem, bind\n",
      "topic 11: converg, represent, network, imag, step\n",
      "topic 12: algorithm, regret, onlin, emb, perform\n",
      "topic 13: loss, network, featur, deep, initi\n",
      "topic 14: agent, memori, fair, data, generat\n",
      "topic 15: method, deep, train, variabl, process\n",
      "topic 16: label, cluster, uncertainti, supervis, problem\n",
      "topic 17: approxim, network, architectur, work, input\n",
      "topic 18: distribut, attent, action, polici, method\n",
      "topic 19: optim, algorithm, function, analysi, theoret\n",
      "topic 20: gradient, approxim, stochast, algorithm, game\n",
      "topic 21: sampl, distribut, kernel, graph, gaussian\n",
      "topic 22: regress, method, estim, linear, base\n",
      "topic 23: class, data, transfer, mean, domain\n",
      "topic 24: data, function, generat, set, test\n",
      "topic 25: train, generat, function, network, object\n",
      "topic 26: method, cluster, optim, valu, algorithm\n",
      "topic 27: algorithm, regret, import, state, sequenc\n",
      "topic 28: estim, convolut, observ, comput, process\n",
      "topic 29: polici, network, neural, perform, task\n",
      "topic 30: convex, set, non, linear, strong\n",
      "topic 31: data, activ, perform, convolut, new\n"
     ]
    }
   ],
   "source": [
    "for topic, words in enumerate(rank_words_per_topic(lambdas, eta, num_topics, num_words)):\n",
    "    print(f'topic {topic}: ' + ', '.join([dictionary[id] for id in words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Gibbs and Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7:** Choose one of the abstracts, present the top 5 topics of the document and present the title of the 5 closest other abstracts. Do this for both of the algorithms on the same abstract. Discuss similairities and differences between the results from the two algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Self-supervised Learning of Motion Capture\n",
      "Abstract: Current state-of-the-art solutions for motion capture from a single camera are optimization driven: they optimize the parameters of a 3D human model so that its re-projection matches measurements in the video (e.g. person segmentation, optical flow, keypoint detections etc.). Optimization models are susceptible to local minima. This has been the bottleneck that forced using clean green-screen like backgrounds at capture time, manual initialization, or switching to multiple cameras as input resource. In this work, we propose a learning based motion capture model for single camera input. Instead of optimizing mesh and skeleton parameters directly, our model optimizes neural network weights that predict 3D shape and skeleton configurations given a monocular RGB video. Our model is trained using a combination of strong supervision from synthetic data, and self-supervision from differentiable rendering of (a) skeletal keypoints, (b) dense 3D mesh motion, and (c) human-background segmentation, in an end-to-end framework. Empirically we show our model combines the best of both worlds of supervised learning and test-time optimization: supervised learning initializes the model parameters in the right regime, ensuring good pose and surface initialization at test time, without manual effort. Self-supervision by back-propagating through differentiable rendering allows (unsupervised) adaptation of the model to the test data, and offers much tighter fit than a pretrained fixed model. We show that the proposed model improves with experience and converges to low-error solutions where previous optimization methods fail.\n",
      "Gibbs Topics:\n",
      "topic 25, aliment: 0.33, neural, dynam, activ, time, neuron\n",
      "topic 0, aliment: 0.21, state, art, base, method, predict\n",
      "topic 6, aliment: 0.13, converg, convex, gradient, stochast, descent\n",
      "topic 14, aliment: 0.10, object, end, set, invari, constraint\n",
      "topic 21, aliment: 0.08, test, result, featur, algorithm, addit\n",
      "Gibbs Document Similarity:\n",
      "Doc 352, similarity: 3.17, title: Sparse convolutional coding for neuronal assembly detection\n",
      "Doc 95, similarity: 3.27, title: Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples\n",
      "Doc 30, similarity: 3.54, title: MaskRNN: Instance Level Video Object Segmentation\n",
      "Doc 539, similarity: 3.88, title: Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model\n",
      "Doc 82, similarity: 4.00, title: Multiscale Semi-Markov Dynamics for Intracortical Brain-Computer Interfaces\n",
      "\n",
      "CAVI Topics:\n",
      "topic 4, aliment: 0.14, time, result, latent, imag, variabl\n",
      "topic 5, aliment: 0.13, neural, network, train, data, deep\n",
      "topic 6, aliment: 0.13, data, rank, estim, generat, matrix\n",
      "topic 3, aliment: 0.13, optim, problem, gradient, larg, approxim\n",
      "topic 1, aliment: 0.12, inform, task, imag, sequenc, likelihood\n",
      "CAVI Document similarity:\n",
      "Doc 643, similarity: 3.94, title: Learning Hierarchical Information Flow with Recurrent Neural Modules\n",
      "Doc 509, similarity: 4.10, title: Identification of Gaussian Process State Space Models\n",
      "Doc 70, similarity: 4.66, title: Learning Efficient Object Detection Models with Knowledge Distillation\n",
      "Doc 38, similarity: 4.81, title: Pose Guided Person Image Generation\n",
      "Doc 535, similarity: 4.92, title: What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?\n"
     ]
    }
   ],
   "source": [
    "def topics_for_doc(doc_id, doc_topics, topic_word, eta, num_topics, num_words, top = 5):\n",
    "    topic_dist = doc_topics[doc_id]/doc_topics[doc_id].sum()\n",
    "    # Get topical words for each topic\n",
    "    topic_words = rank_words_per_topic(topic_word, eta, num_topics, num_words, top)\n",
    "    # Sort the topics based on the topic alignment\n",
    "    topic_dist_words = sorted(zip(topic_dist, range(num_topics), topic_words), reverse = True)\n",
    "    return topic_dist_words[:top]\n",
    "\n",
    "def document_similarity(doc_id, doc_topics, top = 5):\n",
    "    thetas = np.zeros_like(doc_topics)\n",
    "    # Calculate the thetas for each document\n",
    "    for d in range(num_docs):\n",
    "        for k in range(num_topics):\n",
    "            thetas[d][k] = (doc_topics[d][k] + alpha)/ (doc_topics[d].sum() * alpha) \n",
    "    # compute the similarity between the document and all other documents\n",
    "    thetas = np.sqrt(thetas)\n",
    "    thetas -= thetas[doc_id]\n",
    "    thetas = thetas**2\n",
    "    similarity = thetas.sum(axis = 1)\n",
    "    similarity[doc_id] = np.inf # Set the similarity to itself to infinity so its not returned\n",
    "    # Sort the documents based on similarity\n",
    "    similarity_index = np.argsort(similarity)\n",
    "    #return the top similar documents\n",
    "    return list(zip(similarity[similarity_index][:top], similarity_index[:top]))\n",
    "    \n",
    "\n",
    "document =502\n",
    "title = documents.iloc[document]['title']\n",
    "abstract = documents.iloc[document]['abstract']\n",
    "\n",
    "print(f'Title: {title}')\n",
    "print(f'Abstract: {abstract}')\n",
    "\n",
    "print('Gibbs Topics:')\n",
    "for topic_score, topic_id, words in topics_for_doc(document, c, ct, eta, num_topics, num_words):\n",
    "    print(f'topic {topic_id}, aliment: {topic_score:.2f}, ' + ', '.join([dictionary[id] for id in words]))\n",
    "print('Gibbs Document Similarity:')\n",
    "for similarity, doc_id in document_similarity(document, c):\n",
    "    print(f'Doc {doc_id}, similarity: {similarity:.2f}, title: {documents.iloc[doc_id][\"title\"]}')\n",
    "print()\n",
    "print('CAVI Topics:')\n",
    "for topic_score, topic_id, words in topics_for_doc(document, gammas, lambdas, eta, num_topics, num_words):\n",
    "    print(f'topic {topic_id}, aliment: {topic_score:.2f}, ' + ', '.join([dictionary[id] for id in words]))\n",
    "print('CAVI Document similarity:')\n",
    "for similarity, doc_id in document_similarity(document, gammas):\n",
    "    print(f'Doc {doc_id}, similarity: {similarity:.2f}, title: {documents.iloc[doc_id][\"title\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8:** Discuss the key conceptual differences between the Gibbs sampler and the CAVI algorithm. What are the pros and cons of each method? Which method do you prefer and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are both approximate Bayesian inference methods, however\n",
    "\n",
    "## Gibbs sampler \n",
    "\n",
    "Gibbs sampler is a **Markov Chain Monte Carlo** method, non-parametric and sampling-based approximation.It generates samples from the posterior distribution by sequentially sampling each variable conditioned on the current values of the other variables.\n",
    "\n",
    "### Pros\n",
    "\n",
    "* Gibbs sampling eventually converges to the true posterior distribution, given enough iterations.\n",
    "\n",
    "### Cons\n",
    "\n",
    "* Not suitable for high-dimensional spaces since it conditioned on the current values of the other variables.\n",
    "\n",
    "* Samples might be correlated\n",
    "\n",
    "## CAVI algorithm\n",
    "\n",
    "CAVI algorithm is a **Variational inference** method, parametric (“deterministic”) approximation. It approximates the posterior distribution by a simpler, factorizable distribution. CAVI iteratively updates each factor of the approximation to maximize ELBO.\n",
    "\n",
    "### Pros\n",
    "\n",
    "* Very efficient \n",
    "\n",
    "* Suitable for high-dimensional spaces\n",
    "\n",
    "### Cons\n",
    "\n",
    "* CAVI can at most approximate a lower bound of the true posterior distribution. In some cases, posterior will be hard to approximate.\n",
    "\n",
    "* Limited to models where the posterior can be approximated by a factorizable distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
