{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "\n",
    "In this part of the assignment you will analyze a data set consisting of abstracts from the 2017 Conference on Neural Information Processing Systems (NeurIPS), The file `papers2017.csv` contans a list of 679 titles and abstracts from the conference proceedings. \n",
    "\n",
    "The task is to compute the posterior distribution in the LDA model that was discussed during the lectures. Set the hyperparameters of the prior to $\\alpha = \\eta = 1$. The number of topics can be set to $K = 5$.\n",
    "\n",
    "Before we can begin with the inference we need to load and pre-process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Arturas.Aleksandraus\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import scipy\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "stemmer = SnowballStemmer('english')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "documents = pd.read_csv('papers2017.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process the data\n",
    "\n",
    "* **Tokenization:** Split each headline into words, lowercase the words and remove punctuation.\n",
    "* **Small words:** Remove all words with less than 3 characters.\n",
    "* **Stopwords:** Remove all stopwords.\n",
    "* **Lemmatize:** Change words in third person to first person and all verbs into present.\n",
    "* **Stem:** Reduce the words to their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizing and stemming\n",
    "def lem_stem(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text,pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in STOPWORDS and len(token) >= 3:\n",
    "            result.append(lem_stem(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to process all of the abstract through this pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['abstract'].map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we create our dictionary by filtering out words with less than 10 appearances and all words which appears in more than half of the documents.\n",
    "\n",
    "We then create a bag of words, where each processed document gets replaced by a list of words and the number of times they appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collapsed Gibbs\n",
    "\n",
    "**Q1:**  For a sweep of the collapsed Gibbs sampler we need to sample from the distribution $p(\\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\eta)$. Where $\\mathbf{z}$ is the topic per word for the documents.\n",
    "\n",
    "Write out the full distribution and simplify the computations as much as possible. \n",
    "\n",
    "Also explain how to obtain the posterior distribution from the variables $\\mathbf{\\theta}$ (topic proportions for each document) and $\\beta$ (word distributions for each topic), based on the output of the Gibbs sampler. That is how do we find the distribution of these quantities given the distribution of $\\mathbf{z}$.\n",
    "\n",
    "_hint: see the slides_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can analytic compute $\\theta$ and $\\beta$ and only sample $z$. The resulting marginalization leads to a simplify objective function that looks s following:\n",
    "$$\n",
    "p(z_{d,n} = k \\mid \\mathbf{z}^{-(d,n)}, \\mathbf{w}, \\alpha, \\eta) \\propto p(z_{d,n} = k \\mid \\mathbf{z}^{-(d,n)}, \\alpha) \\cdot p(w_{d,n} \\mid z_{d,n} = k, \\mathbf{w}^{-(d,n)}, \\mathbf{z}^{-(d,n)}, \\eta) \\propto \\hat{\\theta}_{d,k} \\hat{\\beta}_{k,v}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{d,k} = \\frac{\\alpha + c_{d,k}}{\\sum_{k'=1}^{K}(\\alpha + c_{d,k'})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{k,v} = \\frac{\\eta + \\tilde{c}_{k,v}}{\\sum_{v'=1}^{V}(\\eta + \\tilde{c}_{k,v'})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2:** Implement the collapsed Gibbs sampler on the provided NeurIPS dataset to approximate the posterior $p(\\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\eta)$. To help with this you have code providing the outer loop, you need to implement the initialization algorithm and the function for calculating the probabilities of the posterior distribution.\n",
    "\n",
    "Where `c` and `ct` are $c$ and $\\tilde{c}$ from the slides. The count of each topic per document and the count of each topic per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization function, sets the topic for each word in each document at random.\n",
    "def initialization_gibbs(bow_corpus, num_topics, num_words):\n",
    "    num_docs = len(bow_corpus)\n",
    "    z = []\n",
    "    c = np.zeros((num_docs,num_topics), dtype = int) # c[d][k] = number of words in document d assigned to topic k\n",
    "    ct = np.zeros((num_topics,num_words), dtype = int)\n",
    "    for d in range(num_docs):\n",
    "        topics_in_doc = np.zeros(0,dtype = int)\n",
    "        for (v,i) in bow_corpus[d]:\n",
    "            # Inner loop to set topic for each copy of a word independently.\n",
    "            for _ in range(i):\n",
    "                # Sample a random topic\n",
    "                k = np.random.randint(num_topics) # how do we sample from categorical distribution? Uniform?\n",
    "                # Update list of topics in document\n",
    "                topics_in_doc = np.append(topics_in_doc,k)\n",
    "\n",
    "                # Update c and ct based on document d, word v and topic k\n",
    "                c[d][k] += 1\n",
    "                ct[k][v] += 1\n",
    "        z.append(topics_in_doc)\n",
    "    return(z, c, ct)\n",
    "# Calculates the probability for each topic on the current word and document.\n",
    "def calc_probs(c, ct, d, v, alpha, eta, num_topics):\n",
    "    p = np.zeros(num_topics)\n",
    "\n",
    "    for k in range(num_topics):\n",
    "        # For each topic calculate the probability that word v in document d belongs to topic k.\n",
    "        theta = (c[d][k] + alpha)/ (c[d].sum() * alpha) \n",
    "        beta = (ct[k][v] + eta) / (ct[k].sum() * eta)\n",
    "        p[k] = theta * beta\n",
    "    # Normalize the vector before returning it.\n",
    "    return p/sum(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "eta = 0.1\n",
    "\n",
    "# Number of iterations (number of samples from the posterior)\n",
    "max_itr_gibbs = 10\n",
    "\n",
    "# Number of topics in the model\n",
    "num_topics = 10\n",
    "\n",
    "# Number of words and documents (may help you later)\n",
    "num_words = len(dictionary)\n",
    "num_docs = len(bow_corpus)\n",
    "\n",
    "# Start by initializing all values: z (topics) should be set randomly and c and ct (tilde c) should be calculated based on the randomly set topics.\n",
    "(z, c, ct) = initialization_gibbs(bow_corpus, num_topics, num_words)\n",
    "\n",
    "# Gibbs sampling:\n",
    "for itr in range(max_itr_gibbs):\n",
    "    for d in range(num_docs):\n",
    "        # indx keeps track of the index of the words in each document\n",
    "        indx = 0\n",
    "        for (v,i) in bow_corpus[d]:\n",
    "            for _ in range(i):\n",
    "                # k is current topic\n",
    "                k = z[d][indx]\n",
    "\n",
    "                # Decrease c and ct based on the current topic\n",
    "                c[d][k] -= 1\n",
    "                ct[k][v] -= 1\n",
    "\n",
    "                # Calculate probabilities for the posterior distribution\n",
    "                probs = calc_probs(c, ct, d, v, alpha, eta, num_topics)\n",
    "                \n",
    "\n",
    "                # Sample new topic\n",
    "                new_k = np.random.choice(num_topics, p = probs)\n",
    "\n",
    "                # Increase c and ct based on the new topic\n",
    "                c[d][new_k] += 1\n",
    "                ct[new_k][v] += 1\n",
    "\n",
    "                # Set the word (index indx) to the new topic\n",
    "                z[d][indx] = new_k\n",
    "                indx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3:** Present the top 5 words based on term-score for each topic, also give a name to each of the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0: converg, algorithm, sampl, distribut, regret\n",
      "topic 1: network, architectur, train, recurr, neural\n",
      "topic 2: train, network, neural, task, layer\n",
      "topic 3: label, function, estim, polici, valu\n",
      "topic 4: algorithm, gradient, function, descent, stochast\n",
      "topic 5: imag, infer, agent, featur, dynam\n",
      "topic 6: data, point, process, generat, attent\n",
      "topic 7: imag, train, generat, gin, distribut\n",
      "topic 8: graph, algorithm, causal, problem, queri\n",
      "topic 9: cluster, algorithm, matrix, kernel, plan\n"
     ]
    }
   ],
   "source": [
    "def beta(v, k, topic_word, eta):\n",
    "    return (topic_word[k][v] + eta) / (topic_word[k].sum() + eta)\n",
    "\n",
    "def term_score(v, k, topic_word, eta, num_topics):  \n",
    "    bkv = beta(v, k, topic_word, eta)\n",
    "    log_beta = np.log(bkv)\n",
    "    log_sum_beta = sum([np.log(beta(v, t, topic_word, eta)) for t in range(num_topics)])\n",
    "    return bkv*(log_beta - log_sum_beta/num_topics)\n",
    "\n",
    "def rank_words_per_topic(topic_word, eta, num_topics, num_words, top = 5):\n",
    "    score_word = []\n",
    "    for k in range(num_topics):\n",
    "        topic_score = []\n",
    "        for v in range(num_words):\n",
    "            topic_score.append((term_score(v, k, topic_word, eta, num_topics), v))\n",
    "        topic_score.sort(reverse = True)\n",
    "        score_word.append(topic_score)\n",
    "    word = [[word for score, word in topic][:top] for topic in score_word]\n",
    "    return word\n",
    "\n",
    "for topic, words in enumerate(rank_words_per_topic(ct, eta, num_topics, num_words)):\n",
    "    print(f'topic {topic}: ' + ', '.join([dictionary[id] for id in words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational inference\n",
    "\n",
    "**Q4:** Write down explicit update expressions for a CAVI algorithm for approximating the posterior distribution $p(\\mathbf{\\theta},\\mathbf{z},\\beta \\mid \\mathbf{w}, \\alpha, \\eta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\phi_{d,n}^k \\propto exp(\\psi(\\gamma_{d,k}) + \\psi(\\lambda_{k,{w_{d,n}}}) - \\psi(\\sum_v \\lambda_{k,v})))\n",
    "$$\n",
    "$$\n",
    "\\gamma_{d} = \\alpha + \\sum_n \\phi_{d,n}\\\\\n",
    "\\lambda_{k} = \\eta + \\sum_d \\sum_n \\mathbb{1}\\{w_{d,n}=v\\}\\phi_{d,n}^k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5:** Implement the CAVI algorithm for the NeurIPS data. To help you with this task some code is provided that you need to fill in.\n",
    "\n",
    "For the code we work with $\\log(\\phi)$, this simplifies some of the expressions and is in general more numericly stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilization function, sets lambdas and gammas randomly, sets all phis to 0.\n",
    "def initialization_cavi(bow_corpus, num_topics, num_words):\n",
    "    num_docs = len(bow_corpus)\n",
    "    logphis = []\n",
    "    wordMatrix = []\n",
    "    docLengths = []\n",
    "    lambdas = np.random.gamma(1, size = (num_topics, num_words))\n",
    "    gammas = np.random.gamma(1, size = (num_docs, num_topics))\n",
    "    for d in range(num_docs):\n",
    "        words_in_doc = 0\n",
    "        word_vec = np.zeros(0, dtype = int)\n",
    "        for (v,i) in bow_corpus[d]:\n",
    "            words_in_doc += i\n",
    "            for _ in range(i):\n",
    "                word_vec = np.append(word_vec, v)\n",
    "        logphis.append(np.zeros((words_in_doc,num_topics)))\n",
    "        docLengths.append(words_in_doc)\n",
    "        wordMatrix.append(word_vec)\n",
    "    return(logphis, lambdas, gammas, docLengths, wordMatrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "eta = 0.1\n",
    "\n",
    "# Number of iterations (number of samples from the posterior)\n",
    "max_itr_cavi = 10\n",
    "\n",
    "# Number of topics in the model\n",
    "num_topics = 10\n",
    "\n",
    "# Number of words and documents (may help you later)\n",
    "num_words = len(dictionary)\n",
    "num_docs = len(bow_corpus)\n",
    "\n",
    "# Start by initializing all values, we set all phis to zero and randomize lambdas and gammas from the gamma distribution. Also calculates doc_lengths and word_matrix.\n",
    "(logphis, lambdas, gammas, doc_lengths, word_matrix) = initialization_cavi(bow_corpus, num_topics, num_words)\n",
    "\n",
    "for itr in range(max_itr_cavi):\n",
    "    for d in range(num_docs):\n",
    "        indx = 0\n",
    "        for (v,i) in bow_corpus[d]:\n",
    "            for _ in range(i):\n",
    "                for k in range(num_topics):\n",
    "                    # Calculate each logphi based on the expected value of the natural parametrization.\n",
    "                    # The digamma function is available in scipy.specieal.digamma\n",
    "                    logphis[d][indx][k] = scipy.special.digamma(gammas[d, k]) + scipy.special.digamma(lambdas[k, v]) - scipy.special.digamma(lambdas[k, :].sum())\n",
    "                # Normalize the logphis                                                                                 \n",
    "                logphis[d][indx, :] = logphis[d][indx, :] - np.log(np.exp(logphis[d][indx, :]).sum())\n",
    "                indx += 1\n",
    "        for k in range(num_topics):\n",
    "            # Calculate the gammas based on the phis\n",
    "            gammas[d][k] = alpha + np.exp(logphis[d][:, k]).sum()\n",
    "    for k in range(num_topics):\n",
    "        for v in range(num_words):\n",
    "            # Update the lambdas. \n",
    "            lambdas[k][v] = eta + np.sum([np.exp(logphis[d][word_matrix[d] == v, k]).sum() for d in range(num_docs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6:** Present the top-5 words based on term-score for each topic and also give a name to each of the topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0: approxim, approach, problem, imag, matrix\n",
      "topic 1: estim, predict, label, loss, represent\n",
      "topic 2: train, object, time, bayesian, attent\n",
      "topic 3: data, algorithm, regret, method, structur\n",
      "topic 4: algorithm, cluster, method, sampl, time\n",
      "topic 5: network, task, neural, inform, polici\n",
      "topic 6: perform, time, generat, convex, approach\n",
      "topic 7: problem, algorithm, task, likelihood, perform\n",
      "topic 8: network, neural, base, covari, design\n",
      "topic 9: method, estim, problem, data, paper\n"
     ]
    }
   ],
   "source": [
    "for topic, words in enumerate(rank_words_per_topic(lambdas, eta, num_topics, num_words)):\n",
    "    print(f'topic {topic}: ' + ', '.join([dictionary[id] for id in words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Gibbs and Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7:** Choose one of the abstracts, present the top 5 topics of the document and present the title of the 5 closest other abstracts. Do this for both of the algorithms on the same abstract. Discuss similairities and differences between the results from the two algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Pose Guided Person Image Generation\n",
      "Abstract: This paper proposes the novel Pose Guided Person Generation Network (PG$^2$) that allows to synthesize person images in arbitrary poses, based on an image of that person and a novel pose. Our generation framework PG$^2$ utilizes the pose information explicitly and consists of two key stages: pose integration and image refinement. In the first stage the condition image and the target pose are fed into a U-Net-like network to generate an initial but coarse image of the person with the target pose. The second stage then refines the initial and blurry result by training a U-Net-like generator in an adversarial way. Extensive experimental results on both 128$\\times$64 re-identification images and 256$\\times$256 fashion photos show that our model generates high-quality person images with convincing details.\n",
      "Gibbs Topics:\n",
      "topic 7, aliment: 0.42, imag, train, generat, gin, distribut\n",
      "topic 5, aliment: 0.31, imag, infer, agent, featur, dynam\n",
      "topic 6, aliment: 0.19, data, point, process, generat, attent\n",
      "topic 9, aliment: 0.04, cluster, algorithm, matrix, kernel, plan\n",
      "topic 8, aliment: 0.03, graph, algorithm, causal, problem, queri\n",
      "Gibbs Document Similarity:\n",
      "Doc 38, similarity: 0.00, title: Pose Guided Person Image Generation\n",
      "Doc 349, similarity: 0.24, title: A multi-agent reinforcement learning model of common-pool resource appropriation\n",
      "Doc 71, similarity: 0.60, title: One-Sided Unsupervised Domain Mapping\n",
      "Doc 293, similarity: 1.07, title: Tomography of the London Underground: a Scalable Model for Origin-Destination Data\n",
      "Doc 66, similarity: 1.13, title: Unsupervised Image-to-Image Translation Networks\n",
      "\n",
      "CAVI Topics:\n",
      "topic 9, aliment: 0.55, method, estim, problem, data, paper\n",
      "topic 2, aliment: 0.24, train, object, time, bayesian, attent\n",
      "topic 6, aliment: 0.12, perform, time, generat, convex, approach\n",
      "topic 4, aliment: 0.09, algorithm, cluster, method, sampl, time\n",
      "topic 7, aliment: 0.00, problem, algorithm, task, likelihood, perform\n",
      "CAVI Document similarity:\n",
      "Doc 38, similarity: 0.00, title: Pose Guided Person Image Generation\n",
      "Doc 186, similarity: 0.73, title: Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration\n",
      "Doc 612, similarity: 1.16, title: Communication-Efficient Distributed Learning of Discrete Distributions\n",
      "Doc 432, similarity: 1.18, title: Trimmed Density Ratio Estimation\n",
      "Doc 141, similarity: 1.37, title: Regret Analysis for Continuous Dueling Bandit\n"
     ]
    }
   ],
   "source": [
    "def topics_for_doc(doc_id, doc_topics, topic_word, eta, num_topics, num_words, top = 5):\n",
    "    topic_dist = doc_topics[doc_id]/doc_topics[doc_id].sum()\n",
    "    # Get topical words for each topic\n",
    "    topic_words = rank_words_per_topic(topic_word, eta, num_topics, num_words, top)\n",
    "    # Sort the topics based on the topic alignment\n",
    "    topic_dist_words = sorted(zip(topic_dist, range(num_topics), topic_words), reverse = True)\n",
    "    return topic_dist_words[:top]\n",
    "\n",
    "def document_similarity(doc_id, doc_topics, top = 5):\n",
    "    theta = lambda c, d, k : (c[d][k] + alpha)/ (c[d].sum() * alpha) \n",
    "    # for every value  doc_topics apply theta function \n",
    "    thetas = np.zeros_like(doc_topics)\n",
    "    for d in range(num_docs):\n",
    "        for k in range(num_topics):\n",
    "            thetas[d][k] = theta(doc_topics, d, k)\n",
    "    thetas = np.sqrt(thetas)\n",
    "    thetas -= thetas[doc_id]\n",
    "    thetas = thetas**2\n",
    "    similarity = thetas.sum(axis = 1)\n",
    "    # similarity[doc_id] = np.inf # Set the similarity to itself to infinity so its not returned\n",
    "    similarity_index = np.argsort(similarity)\n",
    "    return list(zip(similarity[similarity_index][:top], similarity_index[:top]))\n",
    "    \n",
    "\n",
    "document = 38\n",
    "title = documents.iloc[document]['title']\n",
    "abstract = documents.iloc[document]['abstract']\n",
    "\n",
    "print(f'Title: {title}')\n",
    "print(f'Abstract: {abstract}')\n",
    "\n",
    "print('Gibbs Topics:')\n",
    "for topic_score, topic_id, words in topics_for_doc(document, c, ct, eta, num_topics, num_words):\n",
    "    print(f'topic {topic_id}, aliment: {topic_score:.2f}, ' + ', '.join([dictionary[id] for id in words]))\n",
    "print('Gibbs Document Similarity:')\n",
    "for similarity, doc_id in document_similarity(document, c):\n",
    "    print(f'Doc {doc_id}, similarity: {similarity:.2f}, title: {documents.iloc[doc_id][\"title\"]}')\n",
    "print()\n",
    "print('CAVI Topics:')\n",
    "for topic_score, topic_id, words in topics_for_doc(document, gammas, lambdas, eta, num_topics, num_words):\n",
    "    print(f'topic {topic_id}, aliment: {topic_score:.2f}, ' + ', '.join([dictionary[id] for id in words]))\n",
    "print('CAVI Document similarity:')\n",
    "for similarity, doc_id in document_similarity(document, gammas):\n",
    "    print(f'Doc {doc_id}, similarity: {similarity:.2f}, title: {documents.iloc[doc_id][\"title\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8:** Discuss the key conceptual differences between the Gibbs sampler and the CAVI algorithm. What are the pros and cons of each method? Which method do you prefer and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
