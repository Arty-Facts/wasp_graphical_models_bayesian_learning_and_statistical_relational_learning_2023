{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "\n",
    "In this part of the assignment you will analyze a data set consisting of abstracts from the 2017 Conference on Neural Information Processing Systems (NeurIPS), The file `papers2017.csv` contans a list of 679 titles and abstracts from the conference proceedings. \n",
    "\n",
    "The task is to compute the posterior distribution in the LDA model that was discussed during the lectures. Set the hyperparameters of the prior to $\\alpha = \\eta = 1$. The number of topics can be set to $K = 5$.\n",
    "\n",
    "Before we can begin with the inference we need to load and pre-process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import scipy\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "stemmer = SnowballStemmer('english')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "documents = pd.read_csv('papers2017.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process the data\n",
    "\n",
    "* **Tokenization:** Split each headline into words, lowercase the words and remove punctuation.\n",
    "* **Small words:** Remove all words with less than 3 characters.\n",
    "* **Stopwords:** Remove all stopwords.\n",
    "* **Lemmatize:** Change words in third person to first person and all verbs into present.\n",
    "* **Stem:** Reduce the words to their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizing and stemming\n",
    "def lem_stem(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text,pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in STOPWORDS and len(token) >= 3:\n",
    "            result.append(lem_stem(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to process all of the abstract through this pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['abstract'].map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we create our dictionary by filtering out words with less than 10 appearances and all words which appears in more than half of the documents.\n",
    "\n",
    "We then create a bag of words, where each processed document gets replaced by a list of words and the number of times they appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collapsed Gibbs\n",
    "\n",
    "**Q1:**  For a sweep of the collapsed Gibbs sampler we need to sample from the distribution $p(\\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\eta)$. Where $\\mathbf{z}$ is the topic per word for the documents.\n",
    "\n",
    "Write out the full distribution and simplify the computations as much as possible. \n",
    "\n",
    "Also explain how to obtain the posterior distribution from the variables $\\mathbf{\\theta}$ (topic proportions for each document) and $\\beta$ (word distributions for each topic), based on the output of the Gibbs sampler. That is how do we find the distribution of these quantities given the distribution of $\\mathbf{z}$.\n",
    "\n",
    "_hint: see the slides_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2:** Implement the collapsed Gibbs sampler on the provided NeurIPS dataset to approximate the posterior $p(\\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\eta)$. To help with this you have code providing the outer loop, you need to implement the initialization algorithm and the function for calculating the probabilities of the posterior distribution.\n",
    "\n",
    "Where `c` and `ct` are $c$ and $\\tilde{c}$ from the slides. The count of each topic per document and the count of each topic per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization function, sets the topic for each word in each document at random.\n",
    "def initialization_gibbs(bow_corpus, num_topics, num_words):\n",
    "    num_docs = len(bow_corpus)\n",
    "    z = []\n",
    "    c = np.zeros((num_docs,num_topics), dtype = int)\n",
    "    ct = np.zeros((num_topics,num_words), dtype = int)\n",
    "    for d in range(num_docs):\n",
    "        topics_in_doc = np.zeros(0,dtype = int)\n",
    "        for (v,i) in bow_corpus[d]:\n",
    "            # Inner loop to set topic for each copy of a word independently.\n",
    "            for _ in range(i):\n",
    "                # Sample a random topic\n",
    "                k = \n",
    "                # Update list of topics in document\n",
    "                topics_in_doc = np.append(topics_in_doc,k)\n",
    "\n",
    "                # Update c and ct based on document d, word v and topic k\n",
    "                c[ ][ ] =\n",
    "                ct[ ][ ] =\n",
    "        z.append(topics_in_doc)\n",
    "    return(z, c, ct)\n",
    "\n",
    "# Calculates the probability for each topic on the current word and document.\n",
    "def calc_probs(c, ct, d, v, alpha, eta, num_topics):\n",
    "    p = np.zeros(num_topics)\n",
    "    for k in range(num_topics):\n",
    "        # For each topic calculate the probability that word v in document d belongs to topic k.\n",
    "        p =\n",
    "        \n",
    "    # Normalize the vector before returning it.\n",
    "    return p/sum(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = \n",
    "eta =\n",
    "\n",
    "# Number of iterations (number of samples from the posterior)\n",
    "max_itr_gibbs = \n",
    "\n",
    "# Number of topics in the model\n",
    "num_topics =\n",
    "\n",
    "# Number of words and documents (may help you later)\n",
    "num_words = len(dictionary)\n",
    "num_docs = len(bow_corpus)\n",
    "\n",
    "# Start by initializing all values: z (topics) should be set randomly and c and ct (tilde c) should be calculated based on the randomly set topics.\n",
    "(z, c, ct) = initialization_gibbs(bow_corpus, num_topics, num_words)\n",
    "\n",
    "# Gibbs sampling:\n",
    "for itr in range(max_itr_gibbs):\n",
    "    for d in range(num_docs):\n",
    "        # indx keeps track of the index of the words in each document\n",
    "        indx = 0\n",
    "        for (v,i) in bow_corpus[d]:\n",
    "            for tmp in range(i):\n",
    "                # k is current topic\n",
    "                k = z[d][indx]\n",
    "\n",
    "                # Decrease c and ct based on the current topic\n",
    "                c[d][k] -= 1\n",
    "                ct[k][v] -= 1\n",
    "\n",
    "                # Calculate probabilities for the posterior distribution\n",
    "                probs = calc_probs(c, ct, d, v, alpha, eta, num_topics)\n",
    "\n",
    "                # Sample new topic\n",
    "                new_k = np.random.choice(num_topics, p = probs)\n",
    "\n",
    "                # Increase c and ct based on the new topic\n",
    "                c[d][new_k] += 1\n",
    "                ct[new_k][v] += 1\n",
    "\n",
    "                # Set the word (index indx) to the new topic\n",
    "                z[d][indx] = new_k\n",
    "                indx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3:** Present the top 5 words based on term-score for each topic, also give a name to each of the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational inference\n",
    "\n",
    "**Q4:** Write down explicit update expressions for a CAVI algorithm for approximating the posterior distribution $p(\\mathbf{\\theta},\\mathbf{z},\\beta \\mid \\mathbf{w}, \\alpha, \\eta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5:** Implement the CAVI algorithm for the NeurIPS data. To help you with this task some code is provided that you need to fill in.\n",
    "\n",
    "For the code we work with $\\log(\\phi)$, this simplifies some of the expressions and is in general more numericly stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilization function, sets lambdas and gammas randomly, sets all phis to 0.\n",
    "def initialization_cavi(bow_corpus, num_topics, num_words):\n",
    "    num_docs = len(bow_corpus)\n",
    "    logphis = []\n",
    "    wordMatrix = []\n",
    "    docLengths = []\n",
    "    lambdas = np.random.gamma(1, size = (num_topics, num_words))\n",
    "    gammas = np.random.gamma(1, size = (num_docs, num_topics))\n",
    "    for d in range(num_docs):\n",
    "        words_in_doc = 0\n",
    "        word_vec = np.zeros(0, dtype = int)\n",
    "        for (v,i) in bow_corpus[d]:\n",
    "            words_in_doc += i\n",
    "            for _ in range(i):\n",
    "                word_vec = np.append(word_vec, v)\n",
    "        logphis.append(np.zeros((words_in_doc,num_topics)))\n",
    "        docLengths.append(words_in_doc)\n",
    "        wordMatrix.append(word_vec)\n",
    "    return(logphis, lambdas, gammas, docLengths, wordMatrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = \n",
    "eta =\n",
    "\n",
    "# Number of iterations (number of samples from the posterior)\n",
    "max_itr_cavi = \n",
    "\n",
    "# Number of topics in the model\n",
    "num_topics =\n",
    "\n",
    "# Number of words and documents (may help you later)\n",
    "num_words = len(dictionary)\n",
    "num_docs = len(bow_corpus)\n",
    "\n",
    "# Start by initializing all values, we set all phis to zero and randomize lambdas and gammas from the gamma distribution. Also calculates doc_lengths and word_matrix.\n",
    "(logphis, lambdas, gammas, doc_lengths, word_matrix) = initialization_cavi(bow_corpus, num_topics, num_words)\n",
    "for itr in range(max_itr_cavi):\n",
    "    for d in range(num_docs):\n",
    "        indx = 0\n",
    "        for (v,i) in bow_corpus[d]:\n",
    "            for _ in range(i):\n",
    "                for k in range(num_topics):\n",
    "                    # Calculate each logphi based on the expected value of the natural parametrization.\n",
    "                    # The digamma function is available in scipy.specieal.digamma\n",
    "                    logphis[d][indx][k] = \n",
    "                # Normalize the logphis\n",
    "                logphis[d][indx, :] = \n",
    "                indx += 1\n",
    "        for k in range(num_topics):\n",
    "            # Calculate the gammas based on the phis\n",
    "            gammas[d][k] = \n",
    "    for k in range(num_topics):\n",
    "        for v in range(num_words):\n",
    "            # Update the lambdas. \n",
    "            lambdas[k][v] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6:** Present the top-5 words based on term-score for each topic and also give a name to each of the topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Gibbs and Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7:** Choose one of the abstracts, present the top 5 topics of the document and present the title of the 5 closest other abstracts. Do this for both of the algorithms on the same abstract. Discuss similairities and differences between the results from the two algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8:** Discuss the key conceptual differences between the Gibbs sampler and the CAVI algorithm. What are the pros and cons of each method? Which method do you prefer and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
